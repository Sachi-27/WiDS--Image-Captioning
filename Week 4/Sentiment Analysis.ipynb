{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0878067c",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "### Tools used:\n",
    "Numpy, Pandas, Pytorch, NLTK, Scapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a96c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce722a",
   "metadata": {},
   "source": [
    "### Loading Training Data and its Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a8ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('sentiment analysis_train.csv', encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5d7617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     2626\n",
       "positive    1538\n",
       "negative     701\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0822085d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f47e9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The GeoSolutions technology will leverage Bene...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$ESI on lows, down $1.50 to $2.50 BK a real po...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the Finnish-Russian Chamber of Co...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Swedish buyout firm has sold its remaining...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence Sentiment\n",
       "0  The GeoSolutions technology will leverage Bene...  positive\n",
       "1  $ESI on lows, down $1.50 to $2.50 BK a real po...  negative\n",
       "2  For the last quarter of 2010 , Componenta 's n...  positive\n",
       "3  According to the Finnish-Russian Chamber of Co...   neutral\n",
       "4  The Swedish buyout firm has sold its remaining...   neutral"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37cb2fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"The GeoSolutions technology will leverage Benefon 's GPS solutions by providing Location Based Search Technology , a Communities Platform , location relevant multimedia content and a new and powerful commercial model .\",\n",
       " '$ESI on lows, down $1.50 to $2.50 BK a real possibility')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.Sentence[0], train_data.Sentence[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5943a1e",
   "metadata": {},
   "source": [
    "We need to remove punctuation marks like apostrophe, comma, fullstop, dollar, decimals. These are unnecessary elements of the sentence which we want to analyse for sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752da16",
   "metadata": {},
   "source": [
    "## Idea\n",
    "### Preprocessing Data\n",
    "Tokenization, Removing Stopwords, Lemmatization using Scapy/NLTK\n",
    "### Training \n",
    "Can we use LSTM? <br>\n",
    "Or shall we be happy with Neural Networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753f8005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data.Sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f90aff6",
   "metadata": {},
   "source": [
    "### Tokenisation and Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc4b5ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sabya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english') #They are all in lowercase\n",
    "\n",
    "def tokenise(text):\n",
    "    text = re.sub(r'[^A-Za-z]', ' ', text)\n",
    "    words = [word.lower() for word in text.split() if (word.lower() not in stop_words) and (len(word) >= 2)]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9c77b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geosolutions technology leverage benefon gps solutions providing location based search technology communities platform location relevant multimedia content new powerful commercial model'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenise(train_data.Sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8a39559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus , SysOpen Digia has , in accordance with Chapter 14 Section 21 of the Finnish Companies Act 29.9.1978 - 734 , obtained title to all the shares of Sentera that are to be redeemed .\n",
      "thus sysopen digia accordance chapter section finnish companies act obtained title shares sentera redeemed\n"
     ]
    }
   ],
   "source": [
    "print(train_data.Sentence[56])\n",
    "print(tokenise(train_data.Sentence[56]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d4fb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have made long-term investments in developing the system 's implementation model .\n",
      "made long term investments developing system implementation model\n"
     ]
    }
   ],
   "source": [
    "print(train_data.Sentence[786])\n",
    "print(tokenise(train_data.Sentence[786]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28375c",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12a29e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sabya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95331e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sabya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae964f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c64849e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "187fd39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Initialize spacy ‘en’ model, keeping only component needed for lemmatization and creating an engine:\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd8426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    words = [word.lemma_ for word in nlp(text)]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2521557d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In September 2010 , the Finnish group agreed to buy Danish company Rose Poultry A-S for up to EUR23 .9 m in a combination of cash and stock .\n",
      "--------------\n",
      "september finnish group agreed buy danish company rose poultry eur combination cash stock\n",
      "--------------\n",
      "september finnish group agree buy danish company rise poultry eur combination cash stock\n"
     ]
    }
   ],
   "source": [
    "print(train_data.Sentence[89])\n",
    "print('--------------')\n",
    "print(tokenise(train_data.Sentence[89]))\n",
    "print('--------------')\n",
    "print(lemmatize(tokenise(train_data.Sentence[89])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b273063",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf72e774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4865,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = train_data.Sentence\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aaa679d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return lemmatize(tokenise(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "736c0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = inputs.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6bdfbcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4865,),\n",
       " 'geosolution technology leverage benefon gps solution provide location base search technology community platform location relevant multimedia content new powerful commercial model')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape, inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b758c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = train_data.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "feb8383f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4865,),\n",
       " 0    positive\n",
       " 1    negative\n",
       " 2    positive\n",
       " 3     neutral\n",
       " 4     neutral\n",
       " Name: Sentiment, dtype: object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, outputs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d7ce07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Categorise(text):\n",
    "    if text == 'positive':\n",
    "        return 1\n",
    "    elif text == 'negative':\n",
    "        return 0\n",
    "    elif text == 'neutral':\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1f4dae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = outputs.apply(Categorise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52968843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4865,),\n",
       " 0    1.0\n",
       " 1    0.0\n",
       " 2    1.0\n",
       " 3    0.5\n",
       " 4    0.5\n",
       " Name: Sentiment, dtype: float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape, outputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aa0a6d",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "<ul>\n",
    "    <li>Bag of Words</li>\n",
    "    <li>TF-IDF</li>\n",
    "    <li>Word Embeddings</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fb85dc",
   "metadata": {},
   "source": [
    "### Bag of Words (Using CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f110ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CountVectorizer works on the concept of Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adef1460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f90b79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_sparsed = vectorizer.transform(inputs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87259b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4865, 7873)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_sparsed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81e9d1d",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6a5f2b",
   "metadata": {},
   "source": [
    "### Idea\n",
    "\n",
    "We first split the data in a stratified manner into training and validation data.<br>\n",
    "If neural networks do not produce good enough accuracy or f1_score, we change the model to LSTM? <br>\n",
    "But wait! Our inputs are of variable size. How do we deal with this? Also the inputs are words and not numbers. How to deal with this?<br>\n",
    "We build a neural network model for predicting:\n",
    "<ul>\n",
    "    <li>a value between 0 and 1, this may tell us how much positive and negative are the reviews</li>\n",
    "    <li>output layer consists of three units, 0, 1 and 2, each for positive, negative and neutral</li>\n",
    "<ul> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52a19381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf281063",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting into training and validation datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(inputs_sparsed, outputs, stratify = outputs, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36dd5d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5    2101\n",
      "1.0    1230\n",
      "0.0     561\n",
      "Name: Sentiment, dtype: int64\n",
      "0.5    525\n",
      "1.0    308\n",
      "0.0    140\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts())\n",
    "print(y_valid.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad7a4f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.float32, torch.float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_train = X_train.to(dtype=torch.float32)\n",
    "\n",
    "y_train = torch.from_numpy(y_train.values)\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_train = y_train.to(dtype=torch.float32)\n",
    "\n",
    "X_valid = torch.from_numpy(X_valid)\n",
    "X_valid = X_valid.to(dtype=torch.float32)\n",
    "\n",
    "y_valid = torch.from_numpy(y_valid.values)\n",
    "y_valid = y_valid.reshape(y_valid.shape[0],1)\n",
    "y_valid = y_valid.to(dtype=torch.float32)\n",
    "X_train.dtype, y_train.dtype, X_valid.dtype, y_valid.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "21608bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3892, 7873]),\n",
       " torch.Size([3892, 1]),\n",
       " torch.Size([973, 7873]),\n",
       " torch.Size([973, 1]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde283de",
   "metadata": {},
   "source": [
    "### Neural Networks as a Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b260b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(7873,1000)\n",
    "        self.bn1 = nn.BatchNorm1d(1000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,500)\n",
    "        self.bn2 = nn.BatchNorm1d(500)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(500,250)\n",
    "        self.bn3 = nn.BatchNorm1d(250)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(250,100)\n",
    "        self.bn4 = nn.BatchNorm1d(100)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(100,1)  \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e63742d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train(model, optimizer, criterion, X_train, y_train, batch_size, display_step=None, printing = False):\n",
    "    \n",
    "    i = 0\n",
    "    running_loss = 0\n",
    "    num_batches = math.ceil(len(X_train)/batch_size)\n",
    "    input_batches = [X_train[i*batch_size: (i+1)*batch_size] for i in range(num_batches)]\n",
    "    label_batches = [y_train[i*batch_size: (i+1)*batch_size] for i in range(num_batches)]\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_batches[i])\n",
    "        loss = criterion(outputs, label_batches[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i+=len(input_batches[i])\n",
    "        running_loss += loss.item()\n",
    "        if(display_step):\n",
    "            if(i%display_step == 0):\n",
    "                print(f'After {i} mini-batches:')\n",
    "                print(f'Training Loss: {running_loss/display_step}')\n",
    "                running_loss = 0\n",
    "                print('-------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "86312188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.8)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0add2876",
   "metadata": {},
   "source": [
    "### Neural Networks as a Regression problem using Bag of Words Feature Extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34f5d1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 20 mini-batches:\n",
      "Training Loss: 0.060621725581586364\n",
      "-------------------------\n",
      "After 40 mini-batches:\n",
      "Training Loss: 0.12229592334479093\n",
      "-------------------------\n",
      "After 60 mini-batches:\n",
      "Training Loss: 0.1189728682860732\n",
      "-------------------------\n",
      "After 80 mini-batches:\n",
      "Training Loss: 0.11414233837276697\n",
      "-------------------------\n",
      "After 100 mini-batches:\n",
      "Training Loss: 0.11845557112246752\n",
      "-------------------------\n",
      "After 120 mini-batches:\n",
      "Training Loss: 0.09787253756076097\n",
      "-------------------------\n",
      "After 140 mini-batches:\n",
      "Training Loss: 0.10866744369268418\n",
      "-------------------------\n",
      "After 160 mini-batches:\n",
      "Training Loss: 0.11141766365617514\n",
      "-------------------------\n",
      "After 180 mini-batches:\n",
      "Training Loss: 0.09674658626317978\n",
      "-------------------------\n",
      "After 200 mini-batches:\n",
      "Training Loss: 0.09608124028891325\n",
      "-------------------------\n",
      "After 220 mini-batches:\n",
      "Training Loss: 0.1088142329826951\n",
      "-------------------------\n",
      "After 240 mini-batches:\n",
      "Training Loss: 0.10208424441516399\n",
      "-------------------------\n",
      "After 260 mini-batches:\n",
      "Training Loss: 0.08731566285714507\n",
      "-------------------------\n",
      "After 280 mini-batches:\n",
      "Training Loss: 0.10381759572774171\n",
      "-------------------------\n",
      "After 300 mini-batches:\n",
      "Training Loss: 0.11024040952324868\n",
      "-------------------------\n",
      "After 320 mini-batches:\n",
      "Training Loss: 0.10362608404830098\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.10242384560406208\n",
      "-------------------------\n",
      "After 360 mini-batches:\n",
      "Training Loss: 0.09518846906721593\n",
      "-------------------------\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.09869680237025022\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, X_train, y_train, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "671ea0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def val_predict(model, X_valid, y_valid):\n",
    "    val_preds = model(X_valid)\n",
    "    \n",
    "    # Modify values < 0.4 to 0\n",
    "    val_preds[val_preds < 0.4] = 0\n",
    "\n",
    "    # Modify values between 0.4 and 0.6 to 0.5\n",
    "    val_preds[(val_preds >= 0.4) & (val_preds < 0.6)] = 0.5\n",
    "\n",
    "    # Modify values > 0.6 to 1\n",
    "    val_preds[val_preds >= 0.6] = 1\n",
    "    \n",
    "    y_valid_np = y_valid.detach().numpy()\n",
    "    val_preds_np = val_preds.detach().numpy()\n",
    "\n",
    "    # Convert the class labels to integer values\n",
    "    le = LabelEncoder()\n",
    "    y_valid_int = le.fit_transform(y_valid)\n",
    "    val_preds_int = le.transform(val_preds_np)\n",
    "    \n",
    "    print('Classes:',le.classes_)\n",
    "    print('F1 score:',f1_score(y_valid_int, val_preds_int, average=None))\n",
    "    print('F1 score(micro)):',f1_score(y_valid_int, val_preds_int, average='micro'))\n",
    "    print('F1 score(macro):',f1_score(y_valid_int, val_preds_int, average='macro'))\n",
    "    print('F1 score(Weighted):',f1_score(y_valid_int, val_preds_int, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "85027038",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving this model\n",
    "torch.save(model, 'bow_fnn_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3fdf3",
   "metadata": {},
   "source": [
    "Observed performance is not good even on increasing the complexity of neural network. Moreover, training loss shows weird trends, it doesn't reduce even after training on multiple epochs.<br>\n",
    "\n",
    "We shall henceforth try something else\n",
    "<ul>\n",
    "    <li> We were trying to represent the sentiment of the statement as a number between 0 to 1. 0 being very negative and 1 being very positive. We classify numbers below 0.4 to be negative, above 0.6 to be positive and those in between to be neutral.</li>\n",
    "    <li> But clearly our above method is not yielding very good results. We now solve the problem as a ternary classifier.</li>\n",
    "    <li> We can also try using other methods of feature extraction like TF-IDF, Hashing or Word Embeddings</li>\n",
    "    <li> Even if all of these fails, we would have nothing other than LSTM to try </li>\n",
    "</ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7992cd73",
   "metadata": {},
   "source": [
    "### Neural Networks as a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da42b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(7873,1000)\n",
    "        self.bn1 = nn.BatchNorm1d(1000)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1000,500)\n",
    "        self.bn2 = nn.BatchNorm1d(500)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(500,250)\n",
    "        self.bn3 = nn.BatchNorm1d(250)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(250,100)\n",
    "        self.bn4 = nn.BatchNorm1d(100)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(100,3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc5(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "418d21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net2()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.8)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "252e8b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000],\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        ...,\n",
       "        [0.5000],\n",
       "        [1.0000],\n",
       "        [0.5000]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5de7e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabya\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y_train.unique(return_counts = True)\n",
    "le = LabelEncoder()\n",
    "y_train_transformed = torch.from_numpy(le.fit_transform(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9061e83",
   "metadata": {},
   "source": [
    "### Neural Networks as a Classifier using Bag of Words Feature Extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41280cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 20 mini-batches:\n",
      "Training Loss: 0.6086402773857117\n",
      "-------------------------\n",
      "After 40 mini-batches:\n",
      "Training Loss: 1.017254838347435\n",
      "-------------------------\n",
      "After 60 mini-batches:\n",
      "Training Loss: 0.9572258085012436\n",
      "-------------------------\n",
      "After 80 mini-batches:\n",
      "Training Loss: 0.949922752380371\n",
      "-------------------------\n",
      "After 100 mini-batches:\n",
      "Training Loss: 0.94137644469738\n",
      "-------------------------\n",
      "After 120 mini-batches:\n",
      "Training Loss: 0.8775322496891022\n",
      "-------------------------\n",
      "After 140 mini-batches:\n",
      "Training Loss: 0.9181284129619598\n",
      "-------------------------\n",
      "After 160 mini-batches:\n",
      "Training Loss: 0.8976418614387512\n",
      "-------------------------\n",
      "After 180 mini-batches:\n",
      "Training Loss: 0.8163404822349548\n",
      "-------------------------\n",
      "After 200 mini-batches:\n",
      "Training Loss: 0.8204005360603333\n",
      "-------------------------\n",
      "After 220 mini-batches:\n",
      "Training Loss: 0.8780878186225891\n",
      "-------------------------\n",
      "After 240 mini-batches:\n",
      "Training Loss: 0.8438405826687813\n",
      "-------------------------\n",
      "After 260 mini-batches:\n",
      "Training Loss: 0.8114927738904953\n",
      "-------------------------\n",
      "After 280 mini-batches:\n",
      "Training Loss: 0.8280928403139114\n",
      "-------------------------\n",
      "After 300 mini-batches:\n",
      "Training Loss: 0.8562510937452317\n",
      "-------------------------\n",
      "After 320 mini-batches:\n",
      "Training Loss: 0.861630380153656\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.8228701174259185\n",
      "-------------------------\n",
      "After 360 mini-batches:\n",
      "Training Loss: 0.7905225172638893\n",
      "-------------------------\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.7468529343605042\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, X_train, y_train_transformed, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c1dab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.4130635777978521\n",
      "-------------------------\n",
      "Epoch 2\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.20388914019261536\n",
      "-------------------------\n",
      "Epoch 3\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.1106453762512262\n",
      "-------------------------\n",
      "Epoch 4\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.08415311756543815\n",
      "-------------------------\n",
      "Epoch 5\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.06897903263666912\n",
      "-------------------------\n",
      "Epoch 6\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.05736556642284421\n",
      "-------------------------\n",
      "Epoch 7\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.04543262492730527\n",
      "-------------------------\n",
      "Epoch 8\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.03205742715355499\n",
      "-------------------------\n",
      "Epoch 9\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.017497727293888793\n",
      "-------------------------\n",
      "Epoch 10\n",
      "After 380 mini-batches:\n",
      "Training Loss: 0.011439415106954249\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Epoch {i+1}')\n",
    "    train(model, optimizer, criterion, X_train, y_train_transformed, 10, 380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cbf860a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([973, 3])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds = model(X_valid)\n",
    "val_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f959f9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7945, -1.1400,  1.4530],\n",
       "        [-1.0883, -2.1475,  2.6864],\n",
       "        [-0.3323,  1.4375, -1.1679],\n",
       "        [ 0.8504, -3.6752,  2.3532],\n",
       "        [-2.7431,  5.8775, -2.7935]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7bded694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_output(preds):\n",
    "    return preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20c011c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([973])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_preds_labels = analyse_output(val_preds)\n",
    "val_preds_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44f76112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabya\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([973])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_transformed = torch.from_numpy(le.transform(y_valid))\n",
    "y_valid_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5874289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_f1_values(le, y_valid_int, val_preds_int):\n",
    "    print('Classes:',le.classes_)\n",
    "    print('F1 score:',f1_score(y_valid_int, val_preds_int, average=None))\n",
    "    print('F1 score(micro)):',f1_score(y_valid_int, val_preds_int, average='micro'))\n",
    "    print('F1 score(macro):',f1_score(y_valid_int, val_preds_int, average='macro'))\n",
    "    print('F1 score(Weighted):',f1_score(y_valid_int, val_preds_int, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a8c6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'bow_fnn_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "785e09c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0.  0.5 1. ]\n",
      "F1 score: [0.3099631  0.73364055 0.70847458]\n",
      "F1 score(micro)): 0.6670092497430626\n",
      "F1 score(macro): 0.5840260762991916\n",
      "F1 score(Weighted): 0.6647135598791837\n"
     ]
    }
   ],
   "source": [
    "# Even poorer of very less improvement\n",
    "print_f1_values(le, y_valid_transformed, val_preds_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7a72d",
   "metadata": {},
   "source": [
    "Now that neither works good, we seek a change in vectorizer method of generating feature matrix. We now use the Term frequency Inverse document frequency method of feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b78f4",
   "metadata": {},
   "source": [
    "### Using the TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6458a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(inputs)\n",
    "inputs_sparsed = vectorizer.transform(inputs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a90a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4865, 7873)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_sparsed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0d1d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(inputs_sparsed, outputs, stratify=outputs, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e55ce244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32, torch.float32, torch.float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "X_train = X_train.to(dtype=torch.float32)\n",
    "\n",
    "y_train = torch.from_numpy(y_train.values)\n",
    "y_train = y_train.reshape(y_train.shape[0],1)\n",
    "y_train = y_train.to(dtype=torch.float32)\n",
    "\n",
    "X_valid = torch.from_numpy(X_valid)\n",
    "X_valid = X_valid.to(dtype=torch.float32)\n",
    "\n",
    "y_valid = torch.from_numpy(y_valid.values)\n",
    "y_valid = y_valid.reshape(y_valid.shape[0],1)\n",
    "y_valid = y_valid.to(dtype=torch.float32)\n",
    "X_train.dtype, y_train.dtype, X_valid.dtype, y_valid.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6316517d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3405, 7873]), torch.Size([1460, 7873]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5737725",
   "metadata": {},
   "source": [
    "### Neural Networks as a Regression problem using TFIDF Feature Extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9049161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.11021400428212741\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.09975243503885234\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 2\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.033547146607409505\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.029262912602109066\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 3\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.016892900469932047\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.01584038977230461\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 4\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.011218894563396187\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.010898101038936361\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 5\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.008340882783865227\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.008337328824563884\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 6\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.006640671817791264\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.006888973014429212\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 7\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.005627494163961862\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.006150497256791876\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 8\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.005145255932652884\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.005884190423153889\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 9\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.004887487635577974\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.005835219171678866\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 10\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.004792116512886851\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.0056833778083434\n",
      "-------------------------\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model2 = Net()\n",
    "optimizer = optim.SGD(model2.parameters(), lr = 0.001, momentum = 0.8)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Epoch {i+1}')\n",
    "    train(model2, optimizer, criterion, X_train, y_train, 10, 170)\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "56e7da1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0.  0.5 1. ]\n",
      "F1 score: [0.25365854 0.44552846 0.5578125 ]\n",
      "F1 score(micro)): 0.4678082191780822\n",
      "F1 score(macro): 0.4189998306233062\n",
      "F1 score(Weighted): 0.45346170578572226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabya\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\sabya\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "val_predict(model2, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9c9c7",
   "metadata": {},
   "source": [
    "I was happy to see training loss following a very good trend. But the f1_score comes out to be worser than earlier ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e3d42bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0.  0.5 1. ]\n",
      "F1 score: [0.68735084 0.64295875 0.77489967]\n",
      "F1 score(micro)): 0.7042584434654919\n",
      "F1 score(macro): 0.7017364183989279\n",
      "F1 score(Weighted): 0.6910541809319669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabya\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\sabya\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "val_predict(model2, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9751622",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model2, 'tfidf_fnn_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af41d3",
   "metadata": {},
   "source": [
    "### Neural Networks as a Classifier using TFIDF Feature Extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "032a228a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sabya\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.9183718632249271\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.8397009803968317\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 2\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.39327743421582617\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.2698017226860804\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 3\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.12348547141779871\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.09238244681893026\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 4\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.046765909212477065\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.04883600134840783\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 5\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.026856013453182052\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.02708098326864488\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 6\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.017965150818995693\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.01759016194161685\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 7\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.012894088084645131\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.013103527084047741\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 8\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.01002428832135218\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.010544438338290682\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 9\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.008220279644079068\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.00870770351988647\n",
      "-------------------------\n",
      "----------------------------------\n",
      "Epoch 10\n",
      "After 170 mini-batches:\n",
      "Training Loss: 0.007011973962653429\n",
      "-------------------------\n",
      "After 340 mini-batches:\n",
      "Training Loss: 0.0074999217779430395\n",
      "-------------------------\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model3 = Net2()\n",
    "optimizer = optim.SGD(model3.parameters(), lr = 0.001, momentum = 0.8)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "y_train_transformed = torch.from_numpy(le.transform(y_train))\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'Epoch {i+1}')\n",
    "    train(model3, optimizer, criterion, X_train, y_train_transformed, 10, 170)\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "db6df0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [0.  0.5 1. ]\n",
      "F1 score: [0.66981132 0.89170361 0.93347874]\n",
      "F1 score(micro)): 0.8726027397260274\n",
      "F1 score(macro): 0.8316645552132801\n",
      "F1 score(Weighted): 0.8730068476138289\n"
     ]
    }
   ],
   "source": [
    "val_preds_labels3 = analyse_output(model(X_valid))\n",
    "y_valid_transformed3 = torch.from_numpy(le.transform(y_valid))\n",
    "print_f1_values(le, y_valid_transformed3, val_preds_labels3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee37ad6",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "To perform good on training data, you need to prefer TFIDF over Bag of Words<br>\n",
    "And for better predictions of output you need to use Neural networks as a classifier rather than a regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de4408e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model3, 'tfidf_fnn_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "97d4cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('sentiment analysis_test.csv', encoding='Windows-1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eacb4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the test data\n",
    "test_inputs = test_data.Sentence\n",
    "test_inputs = test_inputs.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1fae3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction using the same vectorizer which we used to fit the training data for tfidf method\n",
    "test_inputs_sparsed = vectorizer.transform(test_inputs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab0a0e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 7873)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inputs_sparsed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "14774dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting test inputs to a form that could be used for predictions\n",
    "X_test = torch.from_numpy(test_inputs_sparsed)\n",
    "X_test = X_test.to(dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf3090af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([977, 7873])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d847a851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting outputs for the test data\n",
    "test_preds = analyse_output(model(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "98a2b099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2, 1, 1, 2, 2, 1, 1, 0, 1, 1, 1, 2, 0, 0, 1, 2, 1, 1, 1, 1, 1, 2,\n",
       "        2, 0, 1, 1, 0, 1, 1, 0, 2, 1, 2, 1, 1, 2, 0, 2, 1, 1, 1, 1, 2, 1, 1, 2,\n",
       "        1, 2, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
       "        2, 1, 1, 2, 2, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 2, 2, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 0, 1, 2, 1, 2, 2, 1, 2, 0, 2, 2, 1, 1, 0,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 0, 1, 2, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 0, 2, 2, 1, 2, 2, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 0,\n",
       "        1, 2, 2, 1, 2, 2, 0, 2, 2, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 0, 1, 2, 0, 1,\n",
       "        2, 0, 2, 0, 0, 2, 1, 0, 1, 2, 0, 0, 0, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2,\n",
       "        1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 0, 2, 1, 2, 0, 1, 1, 2, 0, 1, 1, 0,\n",
       "        2, 2, 0, 0, 2, 1, 1, 1, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 0, 2, 1, 1,\n",
       "        1, 0, 1, 1, 1, 2, 2, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 0, 0, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
       "        2, 2, 1, 1, 2, 0, 1, 0, 2, 2, 0, 2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 2,\n",
       "        1, 2, 1, 0, 1, 1, 1, 1, 1, 2, 1, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 1, 2, 2,\n",
       "        2, 2, 0, 1, 2, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 2, 1, 2, 1,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 2, 1, 0, 1, 1, 1, 2, 2,\n",
       "        1, 1, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 0, 2, 1, 1, 0,\n",
       "        1, 2, 1, 2, 0, 1, 2, 1, 1, 2, 1, 1, 1, 0, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        2, 0, 1, 1, 2, 1, 2, 1, 2, 2, 2, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 2, 0, 1, 1, 2, 0, 1, 1, 0, 1, 1, 1,\n",
       "        2, 0, 2, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 1,\n",
       "        2, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 0, 2, 2, 0, 2, 0, 1, 2,\n",
       "        2, 2, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 2, 2, 1, 0, 1, 0,\n",
       "        2, 1, 1, 0, 2, 2, 1, 1, 1, 2, 1, 0, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2,\n",
       "        0, 1, 1, 1, 1, 1, 1, 0, 1, 2, 2, 2, 2, 1, 0, 1, 0, 1, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 1, 2, 1, 2, 1,\n",
       "        1, 1, 0, 2, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 0,\n",
       "        1, 1, 1, 1, 2, 2, 2, 2, 0, 1, 2, 0, 2, 0, 2, 2, 0, 1, 1, 1, 2, 0, 1, 2,\n",
       "        0, 2, 1, 2, 1, 1, 1, 2, 1, 2, 0, 0, 2, 0, 2, 0, 2, 1, 2, 1, 0, 2, 1, 0,\n",
       "        2, 2, 2, 1, 1, 2, 2, 1, 1, 0, 2, 1, 0, 1, 2, 1, 1, 0, 1, 2, 1, 1, 1, 0,\n",
       "        1, 2, 1, 1, 1, 1, 1, 1, 2, 2, 0, 1, 1, 2, 0, 1, 0, 2, 1, 0, 0, 1, 1, 2,\n",
       "        1, 0, 0, 1, 2, 2, 2, 2, 1, 2, 1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 2, 1, 0, 0, 0, 2, 0, 1, 0, 1, 2, 0, 1, 2, 1, 2,\n",
       "        1, 1, 1, 2, 2, 1, 0, 2, 2, 1, 2, 1, 1, 1, 2, 2, 1, 1, 0, 2, 1, 1, 2, 0,\n",
       "        1, 2, 2, 0, 1, 1, 1, 2, 1, 0, 2, 1, 0, 1, 1, 0, 1, 2, 1, 1, 2, 1, 0, 2,\n",
       "        1, 1, 1, 2, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 2, 2, 1, 1, 0, 0, 1, 0, 2,\n",
       "        2, 2, 0, 1, 1, 2, 0, 2, 0, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1,\n",
       "        2, 1, 2, 1, 2, 1, 2, 2, 0, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0,\n",
       "        1, 2, 2, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1,\n",
       "        2, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "685adff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>977.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.138178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.654701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  977.000000\n",
       "mean     1.138178\n",
       "std      0.654701\n",
       "min      0.000000\n",
       "25%      1.000000\n",
       "50%      1.000000\n",
       "75%      2.000000\n",
       "max      2.000000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_column = pd.DataFrame(test_preds)\n",
    "new_column.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30a5285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Sentiment'] = new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a4cc586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Operating loss totaled EUR 25mn compared to a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Renewed AB InBev Bid for SABMiller Ups Stake i...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rautaruukki Corporation Stock exchange release...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Etteplan targets to employ at least 20 people ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thanks to its extensive industry and operation...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Sentiment\n",
       "0  Operating loss totaled EUR 25mn compared to a ...          0\n",
       "1  Renewed AB InBev Bid for SABMiller Ups Stake i...          2\n",
       "2  Rautaruukki Corporation Stock exchange release...          2\n",
       "3  Etteplan targets to employ at least 20 people ...          1\n",
       "4  Thanks to its extensive industry and operation...          1"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8e96a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.replace(to_replace = 0, value = 'negative')\n",
    "test_data = test_data.replace(to_replace = 1, value = 'neutral')\n",
    "test_data = test_data.replace(to_replace = 2, value = 'positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "34fbfefc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral     540\n",
       "positive    286\n",
       "negative    151\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "823dfcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.to_csv('Sentiment Predictions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
